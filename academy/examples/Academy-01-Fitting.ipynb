{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "We load some pythin packages, and define some settings for displaying pandas data frames and suppress common warning in numpy (which can be savely ignored *here*) to make it easier to look at things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from uncertainties import ufloat\n",
    "from uncertainties import unumpy as unp\n",
    "\n",
    "import scipy\n",
    "import scipy.integrate\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ROOT import gSystem\n",
    "#gSystem.Load('libanalysis.so')\n",
    "#from ROOT import Belle2\n",
    "#Belle2.Variable.Manager.Instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from academy.analysis.selection import sig_id_to_label\n",
    "from academy.analysis.plotting import init_plot_style, data_vs_mc_stacked_hist, add_lumi, add_watermark, color_dict, channel_label, add_channel, chi2, create_hist_ratio_figure, set_yscale, create_ylabel, Tango, calculate_ratio\n",
    "from academy.analysis.settings import *\n",
    "from academy.analysis.constants import *\n",
    "\n",
    "from academy.analysis.fitting import run_fit\n",
    "from academy.analysis.systematics import *\n",
    "\n",
    "init_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the data frames we just downloaded, you might have to adjust the path, if you did not put them in the same directoy as the notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = pd.read_hdf('AcademySample.h5')\n",
    "df_data = pd.read_hdf('AcademySampleData.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available columns in the data frame are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in df_mc.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an overview of the decay channels. We usually only differ between channel 15 and 16 (electron and muon), but if you are interesting to split this sample further, you can use the Dstar and D channel id's below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daughter__bo1__cm__spextraInfo__bodecayModeID__bc__bc\n",
    "channels = [\n",
    "    15,  #S B0 -> D*+ e nu\n",
    "    16,  #S B0 -> D*+ mu nu\n",
    "]\n",
    "\n",
    "# daughter__bo1__cm__spdaughter__bo0__cm__spextraInfo__bodecayModeID__bc__bc__bc\n",
    "channels_Dstar = [\n",
    "    23,  #S D*+ -> D0 pi+\n",
    "    24,  #S D*+ -> D+ pi0\n",
    "]  \n",
    "\n",
    "# daughter__bo1__cm__spdaughter__bo0__cm__spdaughter__bo0__cm__spextraInfo__bodecayModeID__bc__bc__bc__bc\n",
    "channels_D = [\n",
    "    31,  #S D+ -> K- pi+ pi+\n",
    "    32,  #N D+ -> K- pi+ pi+ pi0\n",
    "    33,  #N D+ -> K- pi+ pi+ pi+ pi-\n",
    "    34,  #N D+ -> Ks pi+\n",
    "    35,  #N D+ -> Ks pi+ pi0\n",
    "    36,  #N D+ -> Ks pi+ pi+ pi-\n",
    "    37,  #N D+ -> Ks K^+\n",
    "    38,  #N D+ -> K+ K- pi^+\n",
    "    41,  #S D0 -> K- pi+\n",
    "    42,  #S D0 -> K- pi+ pi0\n",
    "    43,  #S D0 -> K- pi+ pi+ pi-\n",
    "    44,  #N D0 -> K- pi+ pi+ pi- pi0\n",
    "    45,  #N D0 -> Ks pi0\n",
    "    46,  #N D0 -> Ks pi+ pi-\n",
    "    47,  #N D0 -> Ks pi+ pi- pi0\n",
    "    48,  #N D0 -> K+ K+\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotoutdir = \"output/Fitting\"\n",
    "try:\n",
    "    os.makedirs(plotoutdir)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_categories = {\n",
    "    \"signal\": [4.1, 4.2],\n",
    "    \"background\": [9, 8, 7, 6, 5, 3],\n",
    "}\n",
    "\n",
    "template_category_names = [\"background\", \"signal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_from_templates(templates, option):\n",
    "    mc_yields = sum(unp.nominal_values(templates[x]) for x in template_category_names)\n",
    "    data_yields = unp.nominal_values(templates[data_type])\n",
    "    mc_uncertainty_sq = sum(unp.std_devs(templates[x])**2 for x in template_category_names)\n",
    "    data_uncertainty_sq = unp.std_devs(templates[data_type])**2\n",
    "    if option == \"asimov_data\" or option == \"toy_data\": \n",
    "        uns_sq = mc_uncertainty_sq + data_uncertainty_sq\n",
    "        return np.sum([x for x in np.nan_to_num((mc_yields - data_yields) ** 2 / uns_sq) if x < 1e7])\n",
    "    if option == \"data\": \n",
    "        chi_p = []\n",
    "        for n_i, nu_i in zip(data_yields, mc_yields):\n",
    "            if n_i > 0: chi_p.append(n_i * np.log(n_i / nu_i) + nu_i - n_i)\n",
    "        chi_p = 2 * np.sum(chi_p)\n",
    "        uns_sq = data_uncertainty_sq\n",
    "        chi = np.sum([x for x in np.nan_to_num((mc_yields - data_yields) ** 2 / uns_sq) if x < 1e7])\n",
    "        return chi_p\n",
    "\n",
    "def ndf_from_templates(templates):\n",
    "    data_yields = unp.nominal_values(templates[data_type])\n",
    "    ndf = sum([n_i > 0 for n_i in data_yields]) - len(template_categories)\n",
    "    return ndf\n",
    "\n",
    "\n",
    "def annotate_gof(ax, templates, ndf, option):\n",
    "    chi2 = chi2_from_templates(templates, option)\n",
    "    ndf = ndf_from_templates(templates)\n",
    "    p_value = scipy.integrate.quad(lambda x: scipy.stats.chi2.pdf(x, df=ndf), chi2, np.inf)[0]\n",
    "    add_channel(ax[0], r\"$\\mathbf{\\chi^2}_\\mathrm{P}$ / ndf = \" + f\"{chi2:.2f} / {ndf}\", px=0.03, py=0.93, fontsize=10)\n",
    "    add_channel(ax[0], r\"p-Value = \" + f\"{p_value:.4f}\", px=0.03, py=0.88, fontsize=10)\n",
    "\n",
    "    \n",
    "def plot_templates(\n",
    "    templates,\n",
    "    template_categories,\n",
    "    bins,\n",
    "    var_str,\n",
    "    unit,\n",
    "    y_str,\n",
    "    data,\n",
    "    color_dict,\n",
    "    label_dict,\n",
    "    full_systematic_covariance=None\n",
    "    \n",
    "):\n",
    "    fig, ax = create_hist_ratio_figure()\n",
    "    fig.subplots_adjust(hspace=0.1)\n",
    "\n",
    "    bin_width = abs(fit_bins[0:-1] - fit_bins[1:])\n",
    "    bin_centers = (fit_bins[0:-1] + fit_bins[1:])/2\n",
    "\n",
    "    bottom = np.zeros(len(bin_centers))\n",
    "    for category in template_categories:\n",
    "        ax[0].bar(\n",
    "            bin_centers,\n",
    "            unp.nominal_values(templates[category]),\n",
    "            width=bin_width,\n",
    "            bottom=bottom,\n",
    "            label=label_dict[category],\n",
    "            color=color_dict[category],\n",
    "            edgecolor=\"black\", \n",
    "            lw=0.5,\n",
    "        )\n",
    "        bottom += unp.nominal_values(templates[category])\n",
    "\n",
    "    ax[0].errorbar(\n",
    "        bin_centers, \n",
    "        unp.nominal_values(templates[data]),\n",
    "        yerr=unp.std_devs(templates[data]),\n",
    "        marker='.',\n",
    "        color='black', \n",
    "        ls='',\n",
    "        label=label_dict[data],\n",
    "    )\n",
    "    \n",
    "    if full_systematic_covariance is None:\n",
    "        mc_uncertainty = sum([unp.std_devs(templates[category]) for category in template_categories])\n",
    "        systematic_label = \"MC Stat. Unc.\"\n",
    "    else:\n",
    "        mc_uncertainty = (sum([unp.std_devs(templates[category]) for category in template_categories])**2 + full_systematic_covariance.diagonal())**0.5\n",
    "        systematic_label = \"Syst. Unc.\"\n",
    "    \n",
    "    ax[0].bar(\n",
    "        bin_centers,\n",
    "        mc_uncertainty,\n",
    "        width=bin_width,\n",
    "        bottom=bottom - mc_uncertainty/2,\n",
    "        color=\"tab:green\", alpha=0.7, hatch=\"//\",\n",
    "        label=systematic_label,\n",
    "    )\n",
    "    \n",
    "    ax[0].set_ylim(0, ax[0].get_ylim()[1])  # Force ymin=0\n",
    "    set_yscale(ax[0])\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    ax[0].legend(handles[::-1], labels[::-1], frameon=False, fontsize=\"x-small\", ncol=1, loc='upper right')\n",
    "    ax[0].set_ylabel(create_ylabel(y_str, bin_width, unit))\n",
    "    var_str = var_str if var_str else column\n",
    "    x_label = var_str + f\" [{unit}]\" if unit else var_str\n",
    "    \n",
    "    ax[1].set_xlabel(x_label)\n",
    "    ax[1].set_ylabel(r\"Data/MC\")\n",
    "    ax[1].set_axisbelow(True)\n",
    "    ax[1].grid(axis=\"y\")\n",
    "\n",
    "    data_ratio = calculate_ratio(\n",
    "        numerator=unp.nominal_values(templates[data]),\n",
    "        denominator=sum([unp.nominal_values(templates[category]) for category in template_categories]),\n",
    "        numerator_uncertainty=unp.std_devs(templates[data]),\n",
    "        denominator_uncertainty=None\n",
    "    )\n",
    "\n",
    "    mc_ratio = calculate_ratio(\n",
    "        numerator=sum([unp.nominal_values(templates[category]) for category in template_categories]),\n",
    "        denominator=sum([unp.nominal_values(templates[category]) for category in template_categories]),\n",
    "        numerator_uncertainty=mc_uncertainty,\n",
    "        denominator_uncertainty=None\n",
    "    )\n",
    "    \n",
    "    ax[1].errorbar(\n",
    "        bin_centers,\n",
    "        unp.nominal_values(data_ratio),\n",
    "        yerr=unp.std_devs(data_ratio),\n",
    "        ls=\"\",\n",
    "        marker=\".\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "    \n",
    "    ax[1].bar(\n",
    "        bin_centers,\n",
    "        mc_uncertainty/bottom,\n",
    "        width=bin_width,\n",
    "        bottom=1 - mc_uncertainty/bottom/2,\n",
    "        color=\"tab:green\", alpha=1.0, hatch=\"//\",\n",
    "    )\n",
    "    ax[1].set_ylim(bottom=0.6, top=1.4)\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_variable = \"m2RecoilSignalSide_after_smearing\" \n",
    "fit_variable_label = r\"$M_\\mathrm{miss}^2$\"\n",
    "fit_variable_unit = r\"GeV$^2$/$c^4$\"\n",
    "fit_bins = np.array([-1.0, -0.25, 0.25, 0.75, 1.25, 2.0])\n",
    "fit_range = (min(fit_bins), max(fit_bins))\n",
    "fit_vois = [\"wReco\", \"costhetalReco\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "    \"signal\": \"tab:orange\",\n",
    "    \"background\": \"tab:blue\",\n",
    "}\n",
    "label_dict = {\n",
    "    \"signal\": r\"Signal\",\n",
    "    \"background\": r\"Background\",\n",
    "    \"data\": \"Data\",\n",
    "    \"asimov_data\": \"Asimov Data\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can choose if you want to use the data sample or an asimov data set (the latter is useful to validate you fits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_type=\"asimov_data\"\n",
    "data_type=\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotoutdir = os.path.join(plotoutdir, data_type)\n",
    "try:\n",
    "    os.makedirs(plotoutdir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "print(plotoutdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fit of the inclusive mm2 distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating templates from the ntuple\n",
    "\n",
    "Here we generate the required ntuples for an inclusive fit in mm2, with the settings defined above. \n",
    "We also determine the systematic uncertainties from MC statistics and form factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_pre_fit_total = {}\n",
    "\n",
    "systematics_index = []\n",
    "mc_statistics_signal_systematics = {}    \n",
    "mc_statistics_background_systematics = {}\n",
    "ff_BtoDst_signal_systematics = {}\n",
    "\n",
    "for channel in tqdm(channels):\n",
    "    templates_pre_fit_total[channel] = {}\n",
    "    \n",
    "    query = f\"daughter__bo1__cm__spextraInfo__bodecayModeID__bc__bc == {channel} and {fit_range[0]} <= {fit_variable} <= {fit_range[1]}\"\n",
    "\n",
    "    mc = df_mc.query(query)\n",
    "    data = df_data.query(query)\n",
    "\n",
    "    for category in template_categories:\n",
    "\n",
    "        n = mc.query(f\"SIG_ID in {template_categories[category]}\")[fit_variable].values\n",
    "        w = mc.query(f\"SIG_ID in {template_categories[category]}\")[\"__weight_overall__\"].values\n",
    "        bin_index = np.digitize(n, fit_bins)\n",
    "        # We drop the over and underflow bin here\n",
    "        bin_content = np.array([np.sum(w[np.where(bin_index == i)]) for i in range(1, len(fit_bins))])\n",
    "        bin_errors = np.array([np.sqrt(np.sum(w[np.where(bin_index == i)] ** 2)) for i in range(1, len(fit_bins))])\n",
    "\n",
    "        templates_pre_fit_total[channel][category] = unp.uarray(bin_content, bin_errors)\n",
    "\n",
    "    bin_content = sum(templates_pre_fit_total[channel][category] for category in template_categories)\n",
    "    bin_errors = bin_content**0.5\n",
    "    templates_pre_fit_total[channel][\"asimov_data\"] = unp.uarray(unp.nominal_values(bin_content), unp.nominal_values(bin_content)**0.5)\n",
    "    \n",
    "    toy_sample = scipy.stats.poisson.rvs(unp.nominal_values(bin_content))\n",
    "    templates_pre_fit_total[channel][\"toy_data\"] = unp.uarray(toy_sample, toy_sample**0.5)\n",
    "    \n",
    "    bin_content = np.histogram(data[fit_variable], bins=fit_bins, range=fit_range, weights=data[\"__weight_overall__\"])[0]\n",
    "    bin_content = np.array([int(c) for c in bin_content])\n",
    "    bin_errors = bin_content**0.5\n",
    "    templates_pre_fit_total[channel][\"data\"] = unp.uarray(bin_content, bin_errors)\n",
    "    \n",
    "    # --- Systematics ---\n",
    "\n",
    "    systematics_index.append((r\"MC Statistics (Sig.)\", channel_label[channel], ))\n",
    "    subquery = f\"SIG_ID in (4.1, 4.2)\"\n",
    "    sub_mc = mc.query(subquery)\n",
    "    mc_statistics_signal_systematics[channel] = mc_statistics_systematics(sub_mc, fit_variable, fit_bins, fit_range,)\n",
    "\n",
    "    \n",
    "    systematics_index.append((r\"MC Statistics (Bkg.)\", channel_label[channel], ))\n",
    "    subquery = f\"SIG_ID not in (4.1, 4.2)\"\n",
    "    sub_mc = mc.query(subquery)\n",
    "    mc_statistics_background_systematics[channel] = mc_statistics_systematics(sub_mc, fit_variable, fit_bins, fit_range,)\n",
    "    \n",
    "    \n",
    "    systematics_index.append((r\"FF $B\\to D^* \\ell \\nu_\\ell$ (Sig.)\", channel_label[channel], ))\n",
    "    subquery = f\"SIG_ID in (4.1, 4.2)\"\n",
    "    sub_mc = mc.query(subquery)\n",
    "    ff_BtoDst_signal_systematics[channel] = form_factor_systematics(\n",
    "        sub_mc, fit_variable, fit_bins, fit_range,\n",
    "        \"weight_ff_btodst_nom\",\n",
    "        [f\"weight_ff_btodst_var_{i}_up\" for i in range(0, 6)],\n",
    "        [f\"weight_ff_btodst_var_{i}_down\" for i in range(0, 6)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_systematics= {}\n",
    "for channel in tqdm(channels):\n",
    "    full_systematics[channel] = {}\n",
    "    full_systematics[channel][\"signal\"] = {}\n",
    "    full_systematics[channel][\"background\"] = {}\n",
    "    \n",
    "    full_systematics[channel][\"signal\"][\"covariance\"] = sum([\n",
    "        mc_statistics_signal_systematics[channel][\"covariance_matrix\"],\n",
    "        ff_BtoDst_signal_systematics[channel][\"covariance_matrix\"],\n",
    "    ])\n",
    "    \n",
    "    full_systematics[channel][\"background\"][\"covariance\"] = sum([\n",
    "        mc_statistics_background_systematics[channel][\"covariance_matrix\"],\n",
    "    ])        \n",
    "        \n",
    "    full_systematics[channel][\"signal\"][\"inv_correlation\"] = np.linalg.inv(full_systematics[channel][\"signal\"][\"covariance\"] / np.outer(\n",
    "        full_systematics[channel][\"signal\"][\"covariance\"].diagonal()**0.5, full_systematics[channel][\"signal\"][\"covariance\"].diagonal()**0.5))\n",
    "    \n",
    "    full_systematics[channel][\"background\"][\"inv_correlation\"] = np.linalg.inv(full_systematics[channel][\"background\"][\"covariance\"] / np.outer(\n",
    "        full_systematics[channel][\"background\"][\"covariance\"].diagonal()**0.5, full_systematics[channel][\"background\"][\"covariance\"].diagonal()**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_covariance_for_plotting = {}\n",
    "\n",
    "for channel in channels:\n",
    "    full_covariance_for_plotting[channel] = sum([\n",
    "        full_systematics[channel][\"signal\"][\"covariance\"],\n",
    "        full_systematics[channel][\"background\"][\"covariance\"],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    templates = templates_pre_fit_total[channel]\n",
    "    fig, ax = plot_templates(\n",
    "        templates=templates,\n",
    "        template_categories=template_category_names,\n",
    "        bins=fit_bins,\n",
    "        var_str=fit_variable_label,\n",
    "        unit=fit_variable_unit,\n",
    "        y_str=\"Entries\",\n",
    "        data=data_type,\n",
    "        color_dict=color_dict,\n",
    "        label_dict=label_dict,\n",
    "        full_systematic_covariance=full_covariance_for_plotting[channel],\n",
    "    )\n",
    "\n",
    "    add_lumi(ax[0], 711)\n",
    "    add_watermark(ax[0], channel_label[channel], fontsize=10)\n",
    "    annotate_gof(ax, templates, len(fit_bins) - 1 - len(template_categories), option=data_type)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plotoutdir}/prefit_channel{channel}_inclusive.pdf\", transparent=True)\n",
    "    plt.savefig(f\"{plotoutdir}/prefit_channel{channel}_inclusive.png\", transparent=True) \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### Exercise 1: Likelihood Functions\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\eta_{k=1,2}, \\vec{\\theta}) = \\prod_{i=1}^\\text{bins} \\mathcal{P}(n_i | \\nu_i(\\theta)) \\times \\prod_k^\\text{templates} \\mathcal{N}(\\theta | 0, \\Sigma^k)\n",
    "\\end{equation}\n",
    "with\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\nu_i(\\theta) &= \\sum_k^\\text{templates} f_{ik}(\\theta_k) \\eta_k \\,,\\\\\n",
    "        f_{ik}(\\theta_k) &= \\frac{\\eta_{ik} (1 + \\theta_{ik} \\epsilon_{ik})}{\\sum_j^\\text{bins} \\eta_{jk} (1 + \\theta_{jk} \\epsilon_{jk})} \\,.\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$\\mathcal{P}(n_i|\\nu_i(\\theta))$ is the Poisson distribution in each bin, depending on the number of measured signal events $n_i$\n",
    "and the MC expectation $\\nu_i(\\theta)$, which depends on nuisance parameters $\\theta$. In each bin, two nuisance parameter contribute:\n",
    "One for the signal and one for the background. This directly defines the number of nuisance parameters to the \n",
    "number of bins times the number of template categories. $\\eta_{ik}$ defines the expected yield of the template $k$ \n",
    "($k\\in\\{\\text{Signal}, \\text{Background}\\}$) in bin $i$ ($\\eta_{ik}$ are not nuisance parameters), and $\\varepsilon_{ik}$ is the associated systematic uncertainty in that bin \n",
    "for that template. $\\mathcal{N}$ is a (number of bins)-dimensional normal distribution, with exepcation value 0 in each dimension\n",
    "and the variance is given by a (number of bins) $\\times$ (number of bins) correlation matrix $\\Sigma^k$ from\n",
    "$C^k_\\mathrm{tot} = \\sum_\\text{sources} C^k_\\text{systematic uncertainty}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from academy.analysis.fitting import prepare_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex.1 a) The first exercise is to define Likelihood functions for the statistical only case and with systematics. \n",
    "\n",
    "The functions have a predefined argument list and return value, so they are compatible with with the rest of the code. \n",
    "We can use the `run_fit` function and pass the Likelihood, that will return the fit result. There is an example execution provided below for testing.  If this executes properly, then the fitting should also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(x, *pars):\n",
    "    \n",
    "    data, templates = x\n",
    "    nBins = len(data)\n",
    "    \n",
    "    # Ex. 1a)\n",
    "    \n",
    "    if np.isposinf(likelihood):  # Catch pathological cases where L is evaluated outside of sound boundaries\n",
    "        return 0\n",
    "    return likelihood\n",
    "\n",
    "# Teste execution of the likelihood function\n",
    "sample, x0 = prepare_fit(templates_pre_fit_total[channel], data_type, template_categories)\n",
    "L(sample, *x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex.1 b) Now we implement the full case which includes the systematic uncertainties\n",
    "\n",
    "The functions have a predefined argument list and return value, so they are compatible with with the rest of the code. \n",
    "We can use the wrapper functions defined below and use the wrapped function with the `run_fit` function and pass the Likelihood, that will return the fit result. There is an example execution provided below for testing. If this executes properly, then the fitting should also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lsys(x,\n",
    "         systematics,\n",
    "         *pars):\n",
    "    # templates[0] = signal\n",
    "    # templates[1] = background\n",
    "    data, (sig, bkg) = x \n",
    "    nBins = len(data)\n",
    "    \n",
    "    nTemplates = 2 \n",
    "    nParamaterOfInterest = nTemplates\n",
    "    nParameterOfNuisance = nTemplates * nBins\n",
    "    par_interest = pars[:nParamaterOfInterest]\n",
    "    par_nuisance = pars[nParamaterOfInterest:nParamaterOfInterest+nParameterOfNuisance]\n",
    "        \n",
    "    par_nuisance_sig = par_nuisance[:nBins]\n",
    "    par_nuisance_bkg = par_nuisance[nBins:]\n",
    "\n",
    "    sig_errors = systematics[0][\"covariance\"].diagonal()**0.5 / unp.nominal_values(sig)\n",
    "    bkg_errors = systematics[1][\"covariance\"].diagonal()**0.5 / unp.nominal_values(bkg)\n",
    "    sig_errors = np.nan_to_num(sig_errors)  # In case for 0 entries in template\n",
    "    bkg_errors = np.nan_to_num(bkg_errors)  # In case for 0 entries in template\n",
    "    \n",
    "    # Ex 1b)\n",
    "    \n",
    "    if np.isposinf(likelihood):  # Catch pathological cases where L is evaluated outside of sound boundaries\n",
    "        return 0\n",
    "    return likelihood\n",
    "\n",
    "# Teste execution of the likelihood function\n",
    "sample, x0 = prepare_fit(templates_pre_fit_total[channel], data_type, template_categories)\n",
    "Lsys(sample, [full_systematics[channel][\"signal\"], full_systematics[channel][\"background\"]], *unp.nominal_values(x0), *np.zeros(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical only fits\n",
    "\n",
    "Here we use our Likelihood function `L`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results_total = {}\n",
    "\n",
    "fit_options = {\n",
    "    \"options\": {\"maxiter\": 200, \"disp\": False},   \n",
    "}\n",
    "\n",
    "for channel in tqdm(channels):\n",
    "    fit_results_total[channel] = {}\n",
    "\n",
    "    fit_result = run_fit(L, templates_pre_fit_total[channel], data_type, template_categories, **fit_options)\n",
    "    fit_results_total[channel] = fit_result\n",
    "    if fit_result.status:\n",
    "        print(f\"{channel}_n/a_{i_bin}_{fit_result.message}\")\n",
    "    if np.isnan(fit_result.covariance).any():\n",
    "        print(f\"{channel}_n/a_{i_bin}_Hesse determination failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical + Systematics fits\n",
    "\n",
    "Here we use our Likelihood function `Lsys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results_total_with_systematics = {}\n",
    "\n",
    "fit_options = {\n",
    "    \"options\": {\"maxiter\": 200, \"disp\": False},   \n",
    "}\n",
    "\n",
    "for channel in tqdm(channels):\n",
    "    fit_results_total_with_systematics[channel] = {}\n",
    "    \n",
    "    def Lwrapper(sample, *pars):\n",
    "        return Lsys(sample, [full_systematics[channel][\"signal\"], full_systematics[channel][\"background\"]], *pars)\n",
    "    x0nuisance = unp.uarray(np.zeros(2*(len(fit_bins)-1)), np.ones(2*(len(fit_bins)-1)))\n",
    "    \n",
    "    fit_result = run_fit(Lwrapper, templates_pre_fit_total[channel], data_type, template_categories, x0nuisance, **fit_options)\n",
    "    fit_results_total_with_systematics[channel] = fit_result\n",
    "    if fit_result.status:\n",
    "        print(f\"{channel}_n/a_{i_bin}_{fit_result.message}\")\n",
    "    if np.isnan(fit_result.covariance).any():\n",
    "        print(f\"{channel}_n/a_{i_bin}_Hesse determination failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Fit Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_post_fit_total = {}\n",
    "\n",
    "signal_nuisance = slice(2, len(fit_bins)+1)\n",
    "background_nuisance = slice(len(fit_bins)+1, 2*len(fit_bins)+2)\n",
    "\n",
    "for channel in channels:\n",
    "    templates_post_fit_total[channel] = {}\n",
    "\n",
    "    for i_yield, category in enumerate(template_categories):\n",
    "        pre_fit = unp.nominal_values(templates_pre_fit_total[channel][category])\n",
    "        relative_errors = full_systematics[channel][category][\"covariance\"].diagonal()**0.5 / pre_fit\n",
    "        relative_errors = np.nan_to_num(relative_errors)\n",
    "        pull = 1 + fit_results_total_with_systematics[channel].x[signal_nuisance] * relative_errors\n",
    "        pre_fit_times_pull = (pre_fit * pull)\n",
    "        fractions = (pre_fit_times_pull) / sum(pre_fit_times_pull)\n",
    "\n",
    "        templates_post_fit_total[channel][category] = fractions * fit_results_total_with_systematics[channel].x[i_yield]\n",
    "        \n",
    "    templates_post_fit_total[channel][\"data\"] = templates_pre_fit_total[channel][\"data\"]\n",
    "    templates_post_fit_total[channel][\"asimov_data\"] = templates_pre_fit_total[channel][\"asimov_data\"]\n",
    "    templates_post_fit_total[channel][\"toy_data\"] = templates_pre_fit_total[channel][\"toy_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_covariance_for_plotting = {}\n",
    "\n",
    "for channel in channels:\n",
    "    full_covariance_for_plotting[channel] = sum([\n",
    "        full_systematics[channel][\"signal\"][\"covariance\"],\n",
    "        full_systematics[channel][\"background\"][\"covariance\"],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "for channel in channels:\n",
    "    fit_result = fit_results_total_with_systematics[channel]\n",
    "    bins = len(fit_bins) - 1\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3), dpi=150)\n",
    "    kwargs = {\n",
    "        \"ls\": \"\",\n",
    "        \"markersize\": 5,\n",
    "        \"markeredgecolor\": 'white',\n",
    "        \"markeredgewidth\": 0.5\n",
    "    }\n",
    "\n",
    "    ax.errorbar(range(0, bins), fit_result.x[2:bins+2], yerr=1, ls='', marker='.', label=\"Pull on Signal Nuisance\", color=\"tab:orange\")\n",
    "    ax.bar(range(0, bins), height=2*np.diagonal(fit_result.covariance[2:bins+2, 2:bins+2])**0.5,\n",
    "           bottom=fit_result.x[2:bins+2]-np.diagonal(fit_result.covariance[2:bins+2, 2:bins+2])**0.5,\n",
    "           width=1, alpha=0.25, label=\"Post-fit Errors\", color=\"tab:orange\")\n",
    "    \n",
    "    ax.errorbar(range(bins, 2*bins), fit_result.x[bins+2:2*bins+2], yerr=1, ls='', marker='.', label=\"Pull on Background Nuisance\", color=\"tab:blue\")\n",
    "    ax.bar(range(bins, 2*bins), height=2*np.diagonal(fit_result.covariance[bins+2:2*bins+2, bins+2:2*bins+2])**0.5,\n",
    "           bottom=fit_result.x[bins+2:2*bins+2]-np.diagonal(fit_result.covariance[bins+2:2*bins+2, bins+2:2*bins+2])**0.5,\n",
    "           width=1, alpha=0.25, label=\"Post-fit Errors\", color=\"tab:blue\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_ylim(-2, 4)\n",
    "    ax.set_xlabel(r\"$\\theta$\")\n",
    "    ax.set_ylabel(\"Standard Deviations\")\n",
    "    ax.set_xticks(np.arange(0, 2*bins, step=3))\n",
    "    ax.legend(loc=\"best\", ncol=1, frameon=False, fontsize=8)\n",
    "\n",
    "    add_watermark(ax, channel_label[channel], fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plotoutdir}/nuisance_channel{channel}_inclusive.pdf\", transparent=True)\n",
    "    plt.savefig(f\"{plotoutdir}/nuisance_channel{channel}_inclusive.png\", transparent=True)     \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields = []\n",
    "index = []\n",
    "for channel in channels:\n",
    "    index.append(channel_label[channel])\n",
    "    yields.append({\n",
    "        \"Channel\": channel_label[channel],\n",
    "        \"Signal Pre-Fit\": ufloat(fit_results_total[channel].x0[0], fit_results_total[channel].covariance0[0, 0]**0.5),\n",
    "        \"Signal Post-Fit\": ufloat(fit_results_total[channel].x[0], fit_results_total[channel].covariance[0, 0]**0.5),\n",
    "        \"Signal Post-Fit w/ Shape\": ufloat(fit_results_total_with_systematics[channel].x[0], fit_results_total_with_systematics[channel].covariance[0, 0]**0.5),\n",
    "        \"Background Pre-Fit\": ufloat(fit_results_total[channel].x0[1], fit_results_total[channel].covariance0[1, 1]**0.5),\n",
    "        \"Background Post-Fit\": ufloat(fit_results_total[channel].x[1], fit_results_total[channel].covariance[1, 1]**0.5),\n",
    "        \"Background Post-Fit w/ Shape\": ufloat(fit_results_total_with_systematics[channel].x[1], fit_results_total_with_systematics[channel].covariance[1, 1]**0.5),\n",
    "        r\"$\\chi^2$ / ndf Pre-Fit\": \"{:.3f} / {}\".format(chi2_from_templates(templates_pre_fit_total[channel], option=data_type), len(fit_bins) - 1 - len(template_categories)),\n",
    "        r\"$\\chi^2$ / ndf Post-Fit\": \"{:.3f} / {}\".format(chi2_from_templates(templates_post_fit_total[channel], option=data_type), len(fit_bins) - 1 - len(template_categories)),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields = pd.DataFrame(yields, index=index)\n",
    "yields[\"Signal Post/Pre\"] = unp.nominal_values(yields[\"Signal Post-Fit\"] / yields[\"Signal Pre-Fit\"])\n",
    "yields[\"Background Post/Pre\"] = unp.nominal_values(yields[\"Background Post-Fit\"] / yields[\"Background Pre-Fit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit in bins of w and cosThetaL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating templates from the ntuple\n",
    "\n",
    "Here we generate the required ntuples for a fit in mm2 in bins of w and cosThetaL, with the settings defined above. \n",
    "We also determine the systematic uncertainties from MC statistics and form factors.\n",
    "\n",
    "Nota Bene: We basically repeat what we already learned! It just looks more complicated, because we have to remember in which bin we are currently working (that is in most cases, just an additional layer of a loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_pre_fit = {}\n",
    "\n",
    "mc_statistics_signal_systematics = {}    \n",
    "mc_statistics_background_systematics = {}\n",
    "ff_BtoDst_signal_systematics = {}\n",
    "\n",
    "for channel in tqdm(channels):\n",
    "    \n",
    "    templates_pre_fit[channel] = {}\n",
    "    \n",
    "    mc_statistics_signal_systematics[channel] = {}    \n",
    "    mc_statistics_background_systematics[channel] = {}\n",
    "    ff_BtoDst_signal_systematics[channel] = {}\n",
    "\n",
    "    for voi in fit_vois:\n",
    "        templates_pre_fit[channel][voi] = {}\n",
    "\n",
    "        mc_statistics_signal_systematics[channel][voi] = {}    \n",
    "        mc_statistics_background_systematics[channel][voi] = {}\n",
    "        ff_BtoDst_signal_systematics[channel][voi] = {}\n",
    "        \n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        for i_bin, voi_bin in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "    \n",
    "\n",
    "            query = f\"daughter__bo1__cm__spextraInfo__bodecayModeID__bc__bc == {channel} and {voi_bin[0]} < {voi} < {voi_bin[1]}\" + f\" and {fit_range[0]} <= {fit_variable} <= {fit_range[1]}\"\n",
    "\n",
    "            mc = df_mc.query(query)\n",
    "            data = df_data.query(query)\n",
    "\n",
    "            templates_pre_fit[channel][voi][i_bin] = {}\n",
    "            for category in template_categories:\n",
    "\n",
    "                n = mc.query(f\"SIG_ID in {template_categories[category]}\")[fit_variable].values\n",
    "                w = mc.query(f\"SIG_ID in {template_categories[category]}\")[\"__weight_overall__\"].values\n",
    "                bin_index = np.digitize(n, fit_bins)\n",
    "                # We drop the over and underflow bin here\n",
    "                bin_content = np.array([np.sum(w[np.where(bin_index == i)]) for i in range(1, len(fit_bins))])\n",
    "                bin_errors = np.array([np.sqrt(np.sum(w[np.where(bin_index == i)] ** 2)) for i in range(1, len(fit_bins))])\n",
    "                \n",
    "                templates_pre_fit[channel][voi][i_bin][category] = unp.uarray(bin_content, bin_errors)\n",
    "\n",
    "            bin_content = sum(templates_pre_fit[channel][voi][i_bin][category] for category in template_categories)\n",
    "            bin_errors = bin_content**0.5\n",
    "            templates_pre_fit[channel][voi][i_bin][\"asimov_data\"] = unp.uarray(unp.nominal_values(bin_content), unp.nominal_values(bin_content)**0.5)\n",
    "            \n",
    "            toy_sample = scipy.stats.poisson.rvs(unp.nominal_values(bin_content))\n",
    "            templates_pre_fit[channel][voi][i_bin][\"toy_data\"] = unp.uarray(toy_sample, toy_sample**0.5)\n",
    "            \n",
    "            bin_content = np.histogram(data[fit_variable], bins=fit_bins, range=fit_range, weights=data[\"__weight_overall__\"])[0]\n",
    "            bin_content = np.array([int(c) for c in bin_content])\n",
    "            bin_errors = bin_content**0.5\n",
    "            templates_pre_fit[channel][voi][i_bin][\"data\"] = unp.uarray(bin_content, bin_errors)\n",
    "            \n",
    "            \n",
    "            # --- Systematics ---\n",
    "            \n",
    "            subquery = f\"SIG_ID in (4.1, 4.2)\"\n",
    "            sub_mc = mc.query(subquery)\n",
    "            mc_statistics_signal_systematics[channel][voi][i_bin] = mc_statistics_systematics(sub_mc, fit_variable, fit_bins, fit_range,)\n",
    "\n",
    "\n",
    "            subquery = f\"SIG_ID not in (4.1, 4.2)\"\n",
    "            sub_mc = mc.query(subquery)\n",
    "            mc_statistics_background_systematics[channel][voi][i_bin] = mc_statistics_systematics(sub_mc, fit_variable, fit_bins, fit_range,)    \n",
    "            \n",
    "            \n",
    "            subquery = f\"SIG_ID in (4.1, 4.2)\"\n",
    "            sub_mc = mc.query(subquery)\n",
    "            ff_BtoDst_signal_systematics[channel][voi][i_bin] = form_factor_systematics(\n",
    "                sub_mc, fit_variable, fit_bins, fit_range,\n",
    "                \"weight_ff_btodst_nom\",\n",
    "                [f\"weight_ff_btodst_var_{i}_up\" for i in range(0, 6)],\n",
    "                [f\"weight_ff_btodst_var_{i}_down\" for i in range(0, 6)],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorrelated_systematics = {}\n",
    "for channel in channels:\n",
    "    uncorrelated_systematics[channel] = {}\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        uncorrelated_systematics[channel][voi] = {}\n",
    "        \n",
    "        for i_bin, _ in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            uncorrelated_systematics[channel][voi][i_bin] = {}\n",
    "            uncorrelated_systematics[channel][voi][i_bin][\"signal\"] = {}\n",
    "            uncorrelated_systematics[channel][voi][i_bin][\"background\"] = {}\n",
    "\n",
    "            uncorrelated_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"] = mc_statistics_signal_systematics[channel][voi][i_bin][\"covariance_matrix\"]\n",
    "            uncorrelated_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"] += np.identity(uncorrelated_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"].shape[0]) * 1e-7  # Sanitize\n",
    "            uncorrelated_systematics[channel][voi][i_bin][\"background\"][\"covariance\"] = mc_statistics_background_systematics[channel][voi][i_bin][\"covariance_matrix\"]\n",
    "            uncorrelated_systematics[channel][voi][i_bin][\"background\"][\"covariance\"] += np.identity(uncorrelated_systematics[channel][voi][i_bin][\"background\"][\"covariance\"].shape[0]) * 1e-7  # Sanitize\n",
    "                \n",
    "\n",
    "            uncorrelated_systematics[channel][voi][i_bin][\"signal\"][\"inv_correlation\"] = np.linalg.inv(uncorrelated_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"] / np.outer(\n",
    "                uncorrelated_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"].diagonal()**0.5, uncorrelated_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"].diagonal()**0.5))\n",
    "\n",
    "            uncorrelated_systematics[channel][voi][i_bin][\"background\"][\"inv_correlation\"] = np.linalg.inv(uncorrelated_systematics[channel][voi][i_bin][\"background\"][\"covariance\"] / np.outer(\n",
    "                uncorrelated_systematics[channel][voi][i_bin][\"background\"][\"covariance\"].diagonal()**0.5, uncorrelated_systematics[channel][voi][i_bin][\"background\"][\"covariance\"].diagonal()**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_systematics = {}\n",
    "for channel in channels:\n",
    "    full_systematics[channel] = {}\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        full_systematics[channel][voi] = {}\n",
    "        \n",
    "        for i_bin, _ in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            full_systematics[channel][voi][i_bin] = {}\n",
    "            full_systematics[channel][voi][i_bin][\"signal\"] = {}\n",
    "            full_systematics[channel][voi][i_bin][\"background\"] = {}\n",
    "\n",
    "            full_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"] = sum([\n",
    "                mc_statistics_signal_systematics[channel][voi][i_bin][\"covariance_matrix\"],\n",
    "            ])\n",
    "            full_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"] += np.identity(full_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"].shape[0]) * 1e-7  # Sanitize\n",
    "\n",
    "            full_systematics[channel][voi][i_bin][\"background\"][\"covariance\"] = sum([\n",
    "                mc_statistics_background_systematics[channel][voi][i_bin][\"covariance_matrix\"]\n",
    "            ])\n",
    "            full_systematics[channel][voi][i_bin][\"background\"][\"covariance\"] += np.identity(full_systematics[channel][voi][i_bin][\"background\"][\"covariance\"].shape[0]) * 1e-7  # Sanitize\n",
    "\n",
    "            full_systematics[channel][voi][i_bin][\"signal\"][\"inv_correlation\"] = np.linalg.inv(full_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"] / np.outer(\n",
    "                full_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"].diagonal()**0.5, full_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"].diagonal()**0.5))\n",
    "\n",
    "            full_systematics[channel][voi][i_bin][\"background\"][\"inv_correlation\"] = np.linalg.inv(full_systematics[channel][voi][i_bin][\"background\"][\"covariance\"] / np.outer(\n",
    "                full_systematics[channel][voi][i_bin][\"background\"][\"covariance\"].diagonal()**0.5, full_systematics[channel][voi][i_bin][\"background\"][\"covariance\"].diagonal()**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_covariance_for_plotting = {}\n",
    "for channel in channels:\n",
    "    full_covariance_for_plotting[channel] = {}\n",
    "    for voi in fit_vois:\n",
    "        full_covariance_for_plotting[channel][voi] = {}\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        for i_bin, _ in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            full_covariance_for_plotting[channel][voi][i_bin] = sum([\n",
    "                full_systematics[channel][voi][i_bin][\"signal\"][\"covariance\"],\n",
    "                full_systematics[channel][voi][i_bin][\"background\"][\"covariance\"],\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        for i_bin, voi_bin in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            templates = templates_pre_fit[channel][voi][i_bin]\n",
    "            fig, ax = plot_templates(\n",
    "                templates=templates,\n",
    "                template_categories=template_category_names,\n",
    "                bins=fit_bins,\n",
    "                var_str=fit_variable_label,\n",
    "                unit=fit_variable_unit,\n",
    "                y_str=\"Entries\",\n",
    "                data=data_type,\n",
    "                color_dict=color_dict,\n",
    "                label_dict=label_dict,\n",
    "                full_systematic_covariance=full_covariance_for_plotting[channel][voi][i_bin],\n",
    "            )\n",
    "\n",
    "            add_lumi(ax[0], 711)\n",
    "            add_watermark(ax[0], channel_label[channel], fontsize=10)\n",
    "            add_channel(ax[0], f\"${voi_bin[0]:.2f} <$ \" + r\"{}\".format(voi_labels[voi]) + f\" $< {voi_bin[1]:.2f}$\", px=0.6, py=0.5)\n",
    "            annotate_gof(ax, templates, len(fit_bins) - 1 - len(template_categories), option=data_type)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{plotoutdir}/prefit_channel{channel}_{fit_variable}_{voi}_{i_bin}.pdf\", transparent=True)\n",
    "            plt.savefig(f\"{plotoutdir}/prefit_channel{channel}_{fit_variable}_{voi}_{i_bin}.png\", transparent=True) \n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical only fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results = {}\n",
    "\n",
    "fit_options = {\n",
    "    \"options\": {\"maxiter\": 200, \"disp\": False},   \n",
    "}\n",
    "\n",
    "for channel in tqdm(channels):\n",
    "    fit_results[channel] = {}\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        fit_results[channel][voi] = {}        \n",
    "        for i_bin, _ in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            fit_result = run_fit(L, templates_pre_fit[channel][voi][i_bin], data_type, template_categories, **fit_options)\n",
    "            fit_results[channel][voi][i_bin] = fit_result\n",
    "            if fit_result.status:\n",
    "                print(f\"{channel}_n/a_{i_bin}_{fit_result.message}\")\n",
    "            if np.isnan(fit_result.covariance).any():\n",
    "                print(f\"{channel}_n/a_{i_bin}_Hesse determination failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical + Systematics fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit with uncorrelated systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results_with_uncorrelated_systematics = {}\n",
    "\n",
    "fit_options = {\n",
    "    \"options\": {\"maxiter\": 200, \"disp\": False},   \n",
    "}\n",
    "\n",
    "for channel in tqdm(channels):\n",
    "    fit_results_with_uncorrelated_systematics[channel] = {}\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        fit_results_with_uncorrelated_systematics[channel][voi] = {}\n",
    "        \n",
    "        for i_bin, _ in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            def Lwrapper(sample, *pars):\n",
    "                return Lsys(sample, [uncorrelated_systematics[channel][voi][i_bin][\"signal\"], uncorrelated_systematics[channel][voi][i_bin][\"background\"]], *pars)\n",
    "            x0nuisance = unp.uarray(np.zeros(2*(len(fit_bins)-1)), np.ones(2*(len(fit_bins)-1)))\n",
    "            \n",
    "            fit_result = run_fit(Lwrapper, templates_pre_fit[channel][voi][i_bin], data_type, template_categories, x0nuisance, **fit_options)\n",
    "            fit_results_with_uncorrelated_systematics[channel][voi][i_bin] = fit_result\n",
    "            if fit_result.status:\n",
    "                print(f\"{channel}_n/a_{i_bin}_{fit_result.message}\")\n",
    "            if np.isnan(fit_result.covariance).any():\n",
    "                print(f\"{channel}_n/a_{i_bin}_Hesse determination failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit with all systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_results_with_systematics = {}\n",
    "\n",
    "fit_options = {\n",
    "    \"options\": {\"maxiter\": 200, \"disp\": False},   \n",
    "}\n",
    "\n",
    "for channel in tqdm(channels):\n",
    "    fit_results_with_systematics[channel] = {}\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        fit_results_with_systematics[channel][voi] = {}\n",
    "        \n",
    "        for i_bin, _ in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            def Lwrapper(sample, *pars):\n",
    "                return Lsys(sample, [full_systematics[channel][voi][i_bin][\"signal\"], full_systematics[channel][voi][i_bin][\"background\"]], *pars)\n",
    "            x0nuisance = unp.uarray(np.zeros(2*(len(fit_bins)-1)), np.ones(2*(len(fit_bins)-1)))\n",
    "            \n",
    "            fit_result = run_fit(Lwrapper, templates_pre_fit[channel][voi][i_bin], data_type, template_categories, x0nuisance, **fit_options)\n",
    "            fit_results_with_systematics[channel][voi][i_bin] = fit_result\n",
    "            if fit_result.status:\n",
    "                print(f\"{channel}_n/a_{i_bin}_{fit_result.message}\")\n",
    "            if np.isnan(fit_result.covariance).any():\n",
    "                print(f\"{channel}_n/a_{i_bin}_Hesse determination failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Fit Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "templates_post_fit = {}\n",
    "\n",
    "signal_nuisance = slice(2, len(fit_bins)+1)\n",
    "background_nuisance = slice(len(fit_bins)+1, 2*len(fit_bins)+2)\n",
    "\n",
    "for channel in channels:\n",
    "    templates_post_fit[channel] = {}\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        templates_post_fit[channel][voi] = {}\n",
    "    \n",
    "        for i_bin in range(len(bin_edges)-1):\n",
    "            templates_post_fit[channel][voi][i_bin] = {}\n",
    "\n",
    "            for i_yield, category in enumerate(template_categories):\n",
    "                pre_fit = unp.nominal_values(templates_pre_fit[channel][voi][i_bin][category])\n",
    "                relative_errors = full_systematics[channel][voi][i_bin][category][\"covariance\"].diagonal()**0.5 / pre_fit\n",
    "                relative_errors = np.nan_to_num(relative_errors)\n",
    "                pull = 1 + fit_results_with_systematics[channel][voi][i_bin].x[signal_nuisance] * relative_errors\n",
    "                pre_fit_times_pull = (pre_fit * pull)\n",
    "                fractions = (pre_fit_times_pull) / sum(pre_fit_times_pull)\n",
    "                \n",
    "                templates_post_fit[channel][voi][i_bin][category] = fractions * fit_results_with_systematics[channel][voi][i_bin].x[i_yield]\n",
    "            \n",
    "            templates_post_fit[channel][voi][i_bin][\"data\"] = templates_pre_fit[channel][voi][i_bin][\"data\"]\n",
    "            templates_post_fit[channel][voi][i_bin][\"toy_data\"] = templates_pre_fit[channel][voi][i_bin][\"toy_data\"]\n",
    "            templates_post_fit[channel][voi][i_bin][\"asimov_data\"] = templates_pre_fit[channel][voi][i_bin][\"asimov_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        for i_bin, voi_bin in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            templates = templates_post_fit[channel][voi][i_bin]\n",
    "            fig, ax = plot_templates(\n",
    "                templates=templates,\n",
    "                template_categories=template_category_names,\n",
    "                bins=fit_bins,\n",
    "                var_str=fit_variable_label,\n",
    "                unit=fit_variable_unit,\n",
    "                y_str=\"Entries\",\n",
    "                data=data_type,\n",
    "                color_dict=color_dict,\n",
    "                label_dict=label_dict,\n",
    "                full_systematic_covariance=full_covariance_for_plotting[channel][voi][i_bin],\n",
    "            )\n",
    "\n",
    "            add_lumi(ax[0], 711)\n",
    "            add_watermark(ax[0], channel_label[channel], fontsize=10)\n",
    "            add_channel(ax[0], f\"${voi_bin[0]:.2f} <$ \" + r\"{}\".format(voi_labels[voi]) + f\" $< {voi_bin[1]:.2f}$\", px=0.6, py=0.5)\n",
    "            annotate_gof(ax, templates, len(fit_bins) - 1 - len(template_categories), option=data_type)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{plotoutdir}/postfit_channel{channel}_{fit_variable}_{voi}_{i_bin}.pdf\", transparent=True)\n",
    "            plt.savefig(f\"{plotoutdir}/postfit_channel{channel}_{fit_variable}_{voi}_{i_bin}.png\", transparent=True) \n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for channel in channels:\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        for i_bin, voi_bin in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            fit_result = fit_results_with_systematics[channel][voi][i_bin]\n",
    "            bins = len(fit_bins) - 1\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(4, 3), dpi=150)\n",
    "            kwargs = {\n",
    "                \"ls\": \"\",\n",
    "                \"markersize\": 5,\n",
    "                \"markeredgecolor\": 'white',\n",
    "                \"markeredgewidth\": 0.5\n",
    "            }\n",
    "\n",
    "            ax.errorbar(range(0, bins), fit_result.x[2:bins+2], yerr=1, ls='', marker='.', label=\"Pull on Signal Nuisance\", color=\"tab:orange\")\n",
    "            ax.bar(range(0, bins), height=2*np.diagonal(fit_result.covariance[2:bins+2, 2:bins+2])**0.5,\n",
    "                   bottom=fit_result.x[2:bins+2]-np.diagonal(fit_result.covariance[2:bins+2, 2:bins+2])**0.5,\n",
    "                   width=1, alpha=0.1, label=\"Post-fit Errors\", color=\"tab:orange\")\n",
    "\n",
    "            ax.errorbar(range(bins, 2*bins), fit_result.x[bins+2:2*bins+2], yerr=1, ls='', marker='.', label=\"Pull on Background Nuisance\", color=\"tab:blue\")\n",
    "            ax.bar(range(bins, 2*bins), height=2*np.diagonal(fit_result.covariance[bins+2:2*bins+2, bins+2:2*bins+2])**0.5,\n",
    "                   bottom=fit_result.x[bins+2:2*bins+2]-np.diagonal(fit_result.covariance[bins+2:2*bins+2, bins+2:2*bins+2])**0.5,\n",
    "                   width=1, alpha=0.1, label=\"Post-fit Errors\", color=\"tab:blue\")\n",
    "\n",
    "            ax.legend()\n",
    "            ax.set_ylim(-2, 4)\n",
    "            ax.set_xlabel(r\"$\\theta$\")\n",
    "            ax.set_ylabel(\"Standard Deviations\")\n",
    "            ax.set_xticks(np.arange(0, 2*bins, step=3))\n",
    "            ax.legend(loc=\"best\", ncol=1, frameon=False, fontsize=8)\n",
    "\n",
    "            add_watermark(ax, channel_label[channel] + \" \" + f\"${voi_bin[0]:.2f} <$ \" + r\"{}\".format(voi_labels[voi]) + f\" $< {voi_bin[1]:.2f}$\" , fontsize=8)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{plotoutdir}/nuisance_channel{channel}_{fit_variable}_{voi}_{i_bin}.pdf\", transparent=True)\n",
    "            plt.savefig(f\"{plotoutdir}/nuisance_channel{channel}_{fit_variable}_{voi}_{i_bin}.png\", transparent=True)     \n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields = []\n",
    "index = []\n",
    "for channel in channels:\n",
    "    for voi in fit_vois:\n",
    "        bin_edges = bin_edges_all[voi]\n",
    "        for i_bin, voi_bin in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "            index.append((channel_label[channel], voi_labels[voi], f\"[{voi_bin[0]:.2f}, {voi_bin[1]:.2f}]\"))\n",
    "            yields.append({\n",
    "                \"$\\nu_\\mathrm{sig}$ Pre\": ufloat(fit_results[channel][voi][i_bin].x0[0], fit_results[channel][voi][i_bin].covariance0[0, 0]**0.5),\n",
    "                \"$\\nu_\\mathrm{sig}$ stat. only\": ufloat(fit_results[channel][voi][i_bin].x[0], fit_results[channel][voi][i_bin].covariance[0, 0]**0.5),\n",
    "                \"$\\nu_\\mathrm{sig}$ stat. + MC stat.\": ufloat(fit_results_with_uncorrelated_systematics[channel][voi][i_bin].x[0], fit_results_with_uncorrelated_systematics[channel][voi][i_bin].covariance[0, 0]**0.5),\n",
    "                \"$\\nu_\\mathrm{sig}$ stat. + MC stat. + shape\": ufloat(fit_results_with_systematics[channel][voi][i_bin].x[0], fit_results_with_systematics[channel][voi][i_bin].covariance[0, 0]**0.5),\n",
    "                \"$\\nu_\\mathrm{bkg}$ Pre\": ufloat(fit_results[channel][voi][i_bin].x0[1], fit_results[channel][voi][i_bin].covariance0[1, 1]**0.5),\n",
    "                \"$\\nu_\\mathrm{bkg}$ stat. only\": ufloat(fit_results[channel][voi][i_bin].x[1], fit_results[channel][voi][i_bin].covariance[1, 1]**0.5),\n",
    "                \"$\\nu_\\mathrm{bkg}$ stat. + MC stat.\": ufloat(fit_results_with_uncorrelated_systematics[channel][voi][i_bin].x[1], fit_results_with_uncorrelated_systematics[channel][voi][i_bin].covariance[1, 1]**0.5),\n",
    "                \"$\\nu_\\mathrm{bkg}$ stat. + MC stat. + shape\": ufloat(fit_results_with_systematics[channel][voi][i_bin].x[1], fit_results_with_systematics[channel][voi][i_bin].covariance[1, 1]**0.5),\n",
    "                r\"$\\chi^2$ Pre\": chi2_from_templates(templates_pre_fit[channel][voi][i_bin], option=data_type),\n",
    "                r\"$\\chi^2$ Post\": chi2_from_templates(templates_post_fit[channel][voi][i_bin], option=data_type),\n",
    "                r\"$\\chi^2$ / ndf Pre\": \"{:.0f} / {}\".format(chi2_from_templates(templates_pre_fit[channel][voi][i_bin], option=data_type), len(fit_bins) - 1),\n",
    "                r\"$\\chi^2$ / ndf Post\": \"{:.0f} / {}\".format(chi2_from_templates(templates_post_fit[channel][voi][i_bin], option=data_type), len(fit_bins) - 1 - len(template_categories)),\n",
    "            })\n",
    "            \n",
    "index = pd.MultiIndex.from_tuples(index, names=[\"Channel\", \"Variable\", \"voi bin\"])\n",
    "yields = pd.DataFrame(yields, index=index)\n",
    "yields[\"$\\nu_\\mathrm{sig}$ Post/Pre\"] = yields[\"$\\nu_\\mathrm{sig}$ stat. + MC stat. + shape\"] / yields[\"$\\nu_\\mathrm{sig}$ Pre\"]\n",
    "yields[\"$\\nu_\\mathrm{bkg}$ Post/Pre\"] = yields[\"$\\nu_\\mathrm{bkg}$ stat. + MC stat. + shape\"] / yields[\"$\\nu_\\mathrm{bkg}$ Pre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields[\"$\\nu_\\mathrm{sig}$\"] = yields[[\"$\\nu_\\mathrm{sig}$ stat. + MC stat. + shape\"]].apply(lambda x: unp.nominal_values(x))\n",
    "yields[\"$\\sigma_\\mathrm{stat}$\"] = yields[[\"$\\nu_\\mathrm{sig}$ stat. only\"]].apply(lambda x: unp.std_devs(x))\n",
    "yields[\"$\\sigma_\\mathrm{MC stat}$\"] = yields[[\"$\\nu_\\mathrm{sig}$ stat. only\", \"$\\nu_\\mathrm{sig}$ stat. + MC stat.\"]].apply(lambda x: (unp.std_devs(x[1])**2 - unp.std_devs(x[0])**2)**0.5, axis=1)\n",
    "yields[\"$\\sigma_\\mathrm{shape}$\"] = yields[[\"$\\nu_\\mathrm{sig}$ stat. + MC stat.\", \"$\\nu_\\mathrm{sig}$ stat. + MC stat. + shape\"]].apply(lambda x: (unp.std_devs(x[1])**2 - unp.std_devs(x[0])**2)**0.5, axis=1)\n",
    "yields[\"$\\sigma_\\mathrm{shape}$\"] = yields[\"$\\sigma_\\mathrm{shape}$\"].replace(np.NaN, 0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FitCorrelations\n",
    "\n",
    "We fit marginalized distributions, so we have to figure out the correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Exercise 2: Bootstrapping Statistical Correlation\n",
    "\n",
    "We determine the statistical correlation of the data by sampling $n$ times with replacement and repeat the fit with the systematics\n",
    "fixed to the best fit value (i.e., we fit only the signal and background yield). With the $n$ fit results we determine the Pearson correlation\n",
    "coefficient. \n",
    "\n",
    "The sample size of the bootstraps has to be chosen such that correlations can be resolved statistically significant down to the correlation coefficients which we want to be resolved.\n",
    "E.g., to resolve stasticical significant correlation coefficients of $r=0.01$, we would need approximately 10'000 bootstraps. In the interest of time, we will work with 100-1000 today.\n",
    "\n",
    "Because we know that the statistical correlation between neighbouring bins of the same differential variable, e.g., $w_i$ and $w_{i+1}$ is 0, we set this to 0\n",
    "instead of using the estimated correlation, which fluctuates around 0 within the 68\\% CL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"data\"\n",
    "number_of_bootstraps = 10  # Tune this number up when you think you are ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. 2 a) Determine the size of the boostrap, which should be a randon number drawn from a poisson distribution, with expectation value of your full data sample. Make use of `scipy.stats.poisson`\n",
    "\n",
    "Ex. 2 b) Sample with replacement, make use of the pandas build in function `DataFrame.sample()`\n",
    "\n",
    "Look in the cell below and fill in the correct lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toys = []\n",
    "\n",
    "df_data_tmp = df_data.query(f\"{fit_range[0]} <= {fit_variable} <= {fit_range[1]}\")[[\"daughter__bo1__cm__spextraInfo__bodecayModeID__bc__bc\", *fit_vois, fit_variable]]\n",
    "\n",
    "for _ in tqdm(range(0, number_of_bootstraps)):\n",
    "    templates_for_correlation = {}\n",
    "    \n",
    "     \n",
    "    bootstrap_size = 0 # <<< Ex. 2a) This should be an integer. scipy.stats.poisson.rvs(len(df_data_tmp))\n",
    "    df_data_boostrap = pd.DataFrame # Ex. 2b) This should be a data frame, a boostrap from your original data sample `df_data_tmp`. \n",
    "    \n",
    "    for channel in channels:\n",
    "        templates_for_correlation[channel] = {}\n",
    "\n",
    "        for voi in fit_vois:\n",
    "            templates_for_correlation[channel][voi] = {}\n",
    "\n",
    "            bin_edges = bin_edges_all[voi]\n",
    "            for i_bin, voi_bin in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "\n",
    "                \n",
    "                query = f\"daughter__bo1__cm__spextraInfo__bodecayModeID__bc__bc == {channel} and {voi_bin[0]} < {voi} < {voi_bin[1]}\"\n",
    "                data = df_data_boostrap.query(query)[[fit_variable]]\n",
    "\n",
    "                templates_for_correlation[channel][voi][i_bin] = {}\n",
    "                for category in template_categories:\n",
    "                    templates_for_correlation[channel][voi][i_bin][category] = templates_post_fit[channel][voi][i_bin][category]\n",
    "\n",
    "                bin_content = np.histogram(data[fit_variable], bins=fit_bins, range=fit_range)[0]\n",
    "                bin_errors = bin_content**0.5\n",
    "                templates_for_correlation[channel][voi][i_bin][\"data\"] = unp.uarray(bin_content, bin_errors)\n",
    "    toys.append(templates_for_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toy_fits = []\n",
    "\n",
    "for toy in tqdm(toys):\n",
    "    fit_results_for_correlation = {}\n",
    "\n",
    "    fit_options = {\n",
    "        \"options\": {\"maxiter\": 200, \"disp\": False},  \n",
    "    }\n",
    "\n",
    "    for channel in channels:\n",
    "        fit_results_for_correlation[channel] = {}\n",
    "        for voi in fit_vois:\n",
    "            bin_edges = bin_edges_all[voi]\n",
    "            fit_results_for_correlation[channel][voi] = {}        \n",
    "            for i_bin, _ in enumerate(zip(bin_edges, bin_edges[1:])):\n",
    "                try:\n",
    "                    fit_result = run_fit(L, toy[channel][voi][i_bin], \"data\", template_categories, calculate_covariance=False, **fit_options)  # We use real data to determine the correlation matrix\n",
    "                    fit_results_for_correlation[channel][voi][i_bin] = fit_result\n",
    "                    if fit_result.status:\n",
    "                        print(f\"{channel}_{voi}_{i_bin}_{fit_result.message}\")\n",
    "                        print(fit_result.x)\n",
    "                except:\n",
    "                    templates = toy[channel][voi][i_bin]\n",
    "                    fig, ax = plot_templates(\n",
    "                        templates=templates,\n",
    "                        template_categories=template_category_names,\n",
    "                        bins=fit_bins,\n",
    "                        var_str=fit_variable_label,\n",
    "                        unit=fit_variable_unit,\n",
    "                        y_str=\"Entries\",\n",
    "                        data=data_type,\n",
    "                        color_dict=color_dict,\n",
    "                        label_dict=label_dict,\n",
    "                        full_systematic_covariance=full_covariance_for_plotting[channel][voi][i_bin],\n",
    "                    )\n",
    "                    add_lumi(ax[0], 711)\n",
    "                    add_watermark(ax[0], channel_label[channel], fontsize=10)\n",
    "                    add_channel(ax[0], f\"${voi_bin[0]:.2f} <$ \" + r\"{}\".format(voi_labels[voi]) + f\" $< {voi_bin[1]:.2f}$\", px=0.6, py=0.5)\n",
    "                    annotate_gof(ax, templates, len(fit_bins) - 1 - len(template_categories))\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "                    \n",
    "    \n",
    "    toy_fits.append(fit_results_for_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. 3 c) From the toy fits, calcultate the correlation coefficient. Make use of numpy's `np.corrcoef`. Because of the way our array is oriented, look at the option `rowvar`.\n",
    "\n",
    "Ex. 4 d) We know there is no statistical correlation between neighbouring bins, only across different distributions (Why?). Set the corresponding elements to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statistical_correlation = {}\n",
    "for channel in channels: \n",
    "    toy_yields = []\n",
    "    for toy_fit in toy_fits:\n",
    "        a = np.array([toy_fit[channel][\"wReco\"][i_bin].x[0] for i_bin in range(10)])\n",
    "        b = np.array([toy_fit[channel][\"costhetalReco\"][i_bin].x[0] for i_bin in range(10)])\n",
    "        toy_yields.append(np.array([*a, *b]))\n",
    "    toy_yields = np.array(toy_yields)\n",
    "    \n",
    "    correlation_matrix = None  # <<< Ex 3 c) Calculate the correlation matrix\n",
    "    \n",
    "    # Ex 3 d) Set the entries we know to be 0 to 0.\n",
    "    \n",
    "    statistical_correlation[channel] = np.clip(correlation_matrix, a_min=0, a_max=None)\n",
    "statistical_correlation[\"full\"] = scipy.linalg.block_diag(*[statistical_correlation[channel] for channel in channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "\n",
    "def plot_correlation(matrix, bin_edges, fit_vois, title, channel=None, source=None):\n",
    "\n",
    "    fig, ax = plt.subplots(dpi=130, figsize=(6.4, 4.4 / 0.8))\n",
    "    im = ax.imshow(matrix, vmin=-1, vmax=1, cmap=plt.get_cmap(\"RdBu_r\"))\n",
    "    \n",
    "    tmp = {}\n",
    "    for voi in fit_vois:\n",
    "        tmp[voi] = np.array([(lower, upper) for lower, upper in zip(bin_edges_all[voi], bin_edges_all[voi][1:])])\n",
    "    tmp = np.array([b for voi in fit_vois for b in tmp[voi]])\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(tmp)))\n",
    "    ax.set_yticks(np.arange(len(tmp)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels([f\"[{lower:.2f}, {upper:.2f}]\" for lower, upper in tmp], rotation=90, fontsize=6)\n",
    "    ax.set_yticklabels([f\"[{lower:.2f}, {upper:.2f}]\" for lower, upper in tmp], fontsize=6)\n",
    "\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(1))\n",
    "\n",
    "    ax.axvline(9.5, color=\"black\")\n",
    "    ax.axvline(19.5, color=\"black\")\n",
    "    \n",
    "    ax.axhline(9.5, color=\"black\")\n",
    "    ax.axhline(19.5, color=\"black\")\n",
    "\n",
    "    ax.text(4, -1.1, r\"$w$\", fontsize=10)\n",
    "    ax.text(13, -1.1, r\"$\\cos \\theta_\\ell$\", fontsize=10)\n",
    "\n",
    "    ax.text(19.6, 4, r\"$w$\", fontsize=10, rotation=-90)\n",
    "    ax.text(19.6, 15, r\"$\\cos \\theta_\\ell$\", fontsize=10, rotation=-90)\n",
    "    \n",
    "    ax.set_title(title, y=1.04)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if channel is not None and source is not None:\n",
    "        plt.savefig(f\"{plotoutdir}/correlation{channel}_{source}.pdf\", transparent=True)\n",
    "        plt.savefig(f\"{plotoutdir}/correlation{channel}_{source}.png\", transparent=True)     \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    plot_correlation(statistical_correlation[channel], bin_edges_all, fit_vois, channel_label[channel], channel, \"statistical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_covariance = {}\n",
    "for channel in channels:\n",
    "    statistical_covariance[channel] = np.outer(yields.loc[channel_label[channel], '$\\sigma_\\mathrm{stat}$'], yields.loc[channel_label[channel], '$\\sigma_\\mathrm{stat}$']) * statistical_correlation[channel]\n",
    "statistical_covariance[\"full\"] = scipy.linalg.block_diag(*[statistical_covariance[channel] for channel in channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape Systematics\n",
    "\n",
    "We treat the shape systematics from other sources (all but MC limited statistics) as fully correlated across the different bins. \n",
    "This systematic uncertainty is negligibly small, so this treatment is a good approximation without any significant influence on the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_systematics_covariance = {}\n",
    "shape_systematics_covariance[\"full\"] = np.outer(yields['$\\sigma_\\mathrm{shape}$'].values, yields['$\\sigma_\\mathrm{shape}$'].values)\n",
    "for i, channel in enumerate(channels):\n",
    "    shape_systematics_covariance[channel] = shape_systematics_covariance[\"full\"][i*40:(i+1)*40, i*40:(i+1)*40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields[\"$\\sigma_\\mathrm{tot}$\"] = yields[[\n",
    "    '$\\sigma_\\mathrm{stat}$',\n",
    "    '$\\sigma_\\mathrm{MC stat}$',\n",
    "    '$\\sigma_\\mathrm{shape}$',\n",
    "]].apply(lambda x: sum(x**2)**0.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yields[[\n",
    "    \"$\\nu_\\mathrm{sig}$\",\n",
    "    \"$\\sigma_\\mathrm{tot}$\",\n",
    "    '$\\sigma_\\mathrm{stat}$',\n",
    "    '$\\sigma_\\mathrm{MC stat}$',\n",
    "    '$\\sigma_\\mathrm{shape}$',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields.to_pickle(os.path.join(plotoutdir, \"yields.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_correlation_of_spectrum = {}\n",
    "stat_correlation_of_spectrum = {}\n",
    "\n",
    "full_correlation_of_spectrum[\"full\"] = sum([statistical_covariance[\"full\"], shape_systematics_covariance[\"full\"]])\n",
    "full_correlation_of_spectrum[\"full\"] = full_correlation_of_spectrum[\"full\"] / np.outer(full_correlation_of_spectrum[\"full\"].diagonal()**0.5, full_correlation_of_spectrum[\"full\"].diagonal()**0.5)\n",
    "stat_correlation_of_spectrum[\"full\"] = statistical_covariance[\"full\"] / np.outer(statistical_covariance[\"full\"].diagonal()**0.5, statistical_covariance[\"full\"].diagonal()**0.5)\n",
    "\n",
    "for i, channel in enumerate(channels):\n",
    "    full_correlation_of_spectrum[channel] = full_correlation_of_spectrum[\"full\"][i*20:(i+1)*20, i*20:(i+1)*20] \n",
    "    stat_correlation_of_spectrum[channel] = stat_correlation_of_spectrum[\"full\"][i*20:(i+1)*20, i*20:(i+1)*20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    plot_correlation(full_correlation_of_spectrum[channel], bin_edges_all, fit_vois, channel_label[channel], channel, f\"full_correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    pd.DataFrame(statistical_correlation[channel]).to_pickle(os.path.join(plotoutdir, f\"correlation_stat_{channel}.pkl\"))\n",
    "    pd.DataFrame(full_correlation_of_spectrum[channel]).to_pickle(os.path.join(plotoutdir, f\"correlation_{channel}.pkl\"))\n",
    "pd.DataFrame(statistical_correlation[\"full\"]).to_pickle(os.path.join(plotoutdir, f\"correlation_stat_full.pkl\"))\n",
    "pd.DataFrame(full_correlation_of_spectrum[\"full\"]).to_pickle(os.path.join(plotoutdir, f\"correlation_full.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = {}\n",
    "for voi in fit_vois:\n",
    "    tmp[voi] = np.array([(lower, upper) for lower, upper in zip(bin_edges_all[voi], bin_edges_all[voi][1:])])\n",
    "tmp = np.array([b for voi in fit_vois for b in tmp[voi]])\n",
    "\n",
    "for channel in channels:\n",
    "    with open(os.path.join(plotoutdir, f\"full_correlation_{channel}.tex\"), \"w\") as f:\n",
    "        # f.write(pd.DataFrame(full_correlation_of_spectrum[channel], columns=[f\"[{lower:.2f}, {upper:.2f}]\" for lower, upper in tmp], index=[f\"[{lower:.2f}, {upper:.2f}]\" for lower, upper in tmp]).to_latex(escape=False))\n",
    "        f.write(pd.DataFrame(full_correlation_of_spectrum[channel]).to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
